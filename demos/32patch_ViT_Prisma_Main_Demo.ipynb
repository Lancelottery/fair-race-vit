{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniajoseph/ViT-Prisma/blob/main/demos/ViT_Prisma_Main_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCO9BA7EqUaE"
      },
      "source": [
        "by Sonia Joseph\n",
        "\n",
        "Twitter: [@soniajoseph_](https://twitter.com/soniajoseph_)\n",
        "\n",
        "# Introduction\n",
        "\n",
        "The purpose of this notebook is to introduce readers to vision transformer (ViT) mechanistic interpretability.\n",
        "\n",
        "To make ViT mechanistic interpretability easier, I built an [open source library Prisma](https://github.com/soniajoseph/ViT-Prisma). The library is based on Neel Nanda's fantastic [TransformerLens](https://github.com/neelnanda-io/TransformerLens) but adapted for vision transformers and text-image models like CLIP. This notebook serves as a demo of the library. I highly encourage readers to check out the library and request features that they'd like to see!\n",
        "\n",
        "I hope this notebook builds the vision mech interp ecosystem and encourages researchers to pursue their own directions with Prisma!\n",
        "\n",
        "## Audience\n",
        "This notebook is geared toward two audiences. The first is familiar with language mech interp, but not vision mech interp. The second is somewhat new to mechanistic interpretability, but is familiar with basic deep learning and has some mild exposure to mech interp concepts. If you are *completely* new to all mech interp, I recommend getting the basics down with the [ARENA curriculum](https://github.com/callummcdougall/ARENA_2.0) first.\n",
        "\n",
        "\n",
        "The structure of this notebook is based on the excellent notebook [Exploratory Analysis](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=lZgu7cH72kdd) from TransformerLens, with some detours. While this notebook acts as a stand-alone, I encourage readers to consult the original notebook when they would like a deeper explanation.\n",
        "\n",
        "For unfamiliar terms, also check out the [mech interp explorer](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#inline-images&theme=default).\n",
        "\n",
        "## Structure\n",
        "\n",
        "*See the sidebar for navigation.*\n",
        "\n",
        "We'll run through the basic mech interp techniques on a vision transformer, including:\n",
        "\n",
        "* Logit attribution\n",
        "* Patch-level emoji logit lens\n",
        "* Attention visualization\n",
        "* Activation patching\n",
        "\n",
        "**We will illustrate the last few technique by changing the ViT's prediction from tabby cat to Border Collie with a minimum viable ablation.**\n",
        "\n",
        "## Acknowledgements and contributors\n",
        "\n",
        "Thank you to Noah MacCallum, Rob Graham, and Karolis Ramanauskas for giving feedback on an early draft of this notebook.\n",
        "\n",
        "Further thank you to Neel Nanda for your feedback, and to the Prisma team and core contributers, the MATS community, and South Park Commons. Full acknowledgements are on the Prisma repo documentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtK4FZb5MyDS"
      },
      "source": [
        "### Differences between ViT and Language Interpretability\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdqCsy85M9cP"
      },
      "source": [
        "*This section is geared toward readers already familiar with language transformer mech interp. If you are new to mech interp in general, you don't have to dwell on this section too much.*\n",
        "\n",
        "\n",
        "Vision mech interp is like language mech interp, but in a fun-house mirror. Both architectures are transformers, so many LLM techniques carry over. However, there are a few twists:\n",
        "\n",
        "* **The typical ViT is not doing unidirectional sequence modeling.** ViTs use bidirectional attention and predict a global CLS token, rather than predicting the next token in an autoregressive manner. (Note: There are autoregressive vision transformers with basically the same architecture as language, such as [Image GPT](https://openai.com/research/image-gpt) and [Parti](https://sites.research.google/parti/), which do next-token image generation. However, as of February 2024, autoregressive vision transformers are not frequently used in the wild.)\n",
        "* **Bidirectional attention vs causal attention.** Language transformers have causal (unidirectional) attention. This means that there is an upper triangular mask on the attention, so that earlier tokens cannot attend to tokens in the future. The classical ViT, with its bidirectional attention, does not have the same concept of \"time.\" Thus, some of the original LLM mech interp techniques break. It can be unclear which direction information is flowing. Induction heads, if they are present in vision, would look different from those in language to account for bidirectional attention.\n",
        "* **CLS token instead of next token prediction/ autoregressive loss.** For ViTs, a learnable CLS token, which is prepended to the input, gets fed into the classification head instead of the final token as in language. The CLS token accrues global information from the other patches through self-attention as all the patches pass through the net.\n",
        "* **No canonical dictionary matrix. Vision is more ambiguous.** Vision lacks a standard dictionary matrix like the 50k one for language, partially due to inherent ambiguity. For instance, a yellow patch on a goldfinch might represent \"yellow,\" \"wing,\" \"goldfinch,\" \"bird,\" or \"animal,\" depending on the granularity, demonstrating hierarchical ambiguity. An animal might be identified specifically as a \"Border collie\" or more generally as a \"dog.\" Beyond hierarchy, ambiguity in vision also stems from cultural interpretations and the imprecision of language. Practically, ImageNet's 1000 classes serve as a makeshift \"dictionary,\" but it falls short of fully encompassing visual concepts.\n",
        "* **Additional hyperparameters.** Patch size is a vision-specific hyperparameter, determining the size of the patches into which an image is divided. Using smaller patches increases accuracy but also computational load, because attention scales quadratically with patch number.\n",
        "* **There is a zoo of vision transformers.** Similar to language, vision transformers come in many forms. The most relevant are the vanilla ViT, which we'll be analyzing in this notebook; CLIP, which is co-trained with text using contrastive loss; and DINO uses unlabeled data. For a review, check out [this survey](https://arxiv.org/pdf/2101.01169.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRmhq7T8iObj"
      },
      "source": [
        "## Import libraries, data, and helper functions (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../src')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MHyAQuZiu3ci"
      },
      "outputs": [],
      "source": [
        "from vit_prisma.utils.data_utils import race_dict\n",
        "from vit_prisma.utils import prisma_utils\n",
        "from vit_prisma.prisma_tools.race_logit_lens import get_patch_logit_directions, get_patch_logit_dictionary\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from fancy_einsum import einsum\n",
        "from collections import defaultdict\n",
        "\n",
        "%matplotlib inline\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from matplotlib import colors as mcolors\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIpV4X9F_3O"
      },
      "source": [
        "**Helper Functions** (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vLuiwSw3_1at"
      },
      "outputs": [],
      "source": [
        "# Helper function (ignore)\n",
        "def plot_image(image):\n",
        "  plt.figure()\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image.permute(1,2,0))\n",
        "\n",
        "class ConvertTo3Channels:\n",
        "    def __call__(self, img):\n",
        "        if img.mode != 'RGB':\n",
        "            return img.convert('RGB')\n",
        "        return img\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    ConvertTo3Channels(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TNUEm2XqF8NF"
      },
      "outputs": [],
      "source": [
        "def plot_logit_boxplot(average_logits, labels):\n",
        "  hovertexts = np.array([[RACE_DICT[i] for _ in range(25)] for i in range(3)])\n",
        "\n",
        "  fig = go.Figure()\n",
        "  data = []\n",
        "\n",
        "  # if tensor, turn to numpy\n",
        "  if isinstance(average_logits, torch.Tensor):\n",
        "      average_logits = average_logits.detach().cpu().numpy()\n",
        "\n",
        "  for i in range(average_logits.shape[1]):  # For each layer\n",
        "      layer_logits = average_logits[:, i]\n",
        "      hovertext = hovertexts[:, i]\n",
        "      box = fig.add_trace(go.Box(\n",
        "          y=layer_logits,\n",
        "          name=f'{layer_labels[i]}',\n",
        "          text=hovertext,\n",
        "          hoverinfo='y+text',\n",
        "          boxpoints='suspectedoutliers'\n",
        "      ))\n",
        "      data.append(box)\n",
        "\n",
        "\n",
        "  means = np.mean(average_logits, axis=0)\n",
        "  fig.add_trace(go.Scatter(\n",
        "      x = layer_labels,\n",
        "      y=means,\n",
        "      mode='markers',\n",
        "      name='Mean',\n",
        "      # line=dict(color='gray'),\n",
        "      marker=dict(size=4, color='red'),\n",
        "  ))\n",
        "\n",
        "\n",
        "  fig.update_layout(\n",
        "      title='Raw Logit Values Per Layer (each dot is 1 ImageNet Class)',\n",
        "      xaxis=dict(title='Layer'),\n",
        "      yaxis=dict(title='Logit Values'),\n",
        "      showlegend=False\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KG3howry0Axq"
      },
      "outputs": [],
      "source": [
        "def plot_patched_component(patched_head, title=''):\n",
        "  \"\"\"\n",
        "  Use for plotting Activation Patching.\n",
        "  \"\"\"\n",
        "\n",
        "  fig = go.Figure(data=go.Heatmap(\n",
        "      z=patched_head.detach().numpy(),\n",
        "      colorscale='RdBu',  # You can choose any colorscale\n",
        "      colorbar=dict(title='Value'),  # Customize the color bar\n",
        "      hoverongaps=False\n",
        "  ))\n",
        "  fig.update_layout(\n",
        "      title=title,\n",
        "      xaxis_title='Attention Head',\n",
        "      yaxis_title='Patch Number',\n",
        "  )\n",
        "\n",
        "  return fig\n",
        "\n",
        "def imshow(tensor, **kwargs):\n",
        "    \"\"\"\n",
        "    Use for Activation Patching.\n",
        "    \"\"\"\n",
        "    px.imshow(\n",
        "          prisma_utils.to_numpy(tensor),\n",
        "          color_continuous_midpoint=0.0,\n",
        "          color_continuous_scale=\"RdBu\",\n",
        "          **kwargs,\n",
        "      ).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAHnpeVuqdzV"
      },
      "source": [
        "# Load model and data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrrRs3CCnn20"
      },
      "source": [
        "## ViT Architecture\n",
        "\n",
        "![image](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\n",
        "\n",
        "\n",
        "A [vision transformer](https://arxiv.org/pdf/2010.11929.pdf) (ViT) is an architecture designed for image classification tasks, similar to the classic transformer architecture used in language models. A ViT consists of transformer blocks; each block consists of an Attention layer and an MLP layer.\n",
        "\n",
        "\n",
        "Unlike language models, vision transformers do not have a dictionary embedding and unembedding matrix. Instead, images are divided into non-overlapping patches, similar to tokens in language models. These patches are flattened and linearly projected to embeddings via a Conv2D layer, similar to word embeddings in language models. A learnable class token (CLS token) is appended to the beginning of the sequence, which accrues global information throughout the network. A linear position embedding is added to the patches.\n",
        "\n",
        "The patch embeddings then pass through the transformer blocks (each block consists of a layer norm, an attention layer, another layernorm, and an mlp layer). The output of each block is added back to the previous input. The sum of the block and the previous input is called the residual stream.\n",
        "\n",
        "The final layer of this vision transformer is a classification head with 1000 logit values for ImageNet's 1000 classes. The CLS token is fed into the final layer for 1000-way classification.\n",
        "\n",
        "Like TransformerLens, we use HookedViT to easily capture intermediate activations with custom hook functions, instead of dealing with PyTorch's normal hook functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "1d79d72f8b6b4a0cb2407a3f555ee1ec",
            "8e7d2706e772426ca3bf653b6fafe91f",
            "ebeaf9ba22934b20ac81fe2770c1ec03",
            "fec789e2132d4878bdb4935ee35c82c6",
            "a27d66e732e2492e832f2326f3252908",
            "b9fb88435e7e4c5f89cc915b7a0856fd",
            "bbc3cbe3b9d44b6d9b93a047a11334ad",
            "62752397a68b465e84032f72f3cca0cc",
            "d41dca76979a4e2e8fda8e05b50a222b",
            "6428af5766574e5fa07c1348a1965be9",
            "5049b576415a4ef4b5c9382b0819d440",
            "a875e084c4c44bce8c2ece41e946b859",
            "a15fd9308e7c4d799cf4622d40ca70d5",
            "4161dece78b147d893b036997d5bffcb",
            "c59ba6dbe4554429806f21ea948cc56d",
            "009df655df6e4edbb5de9a7a318ef411",
            "52f59766c812422288ae91e2cbfdf95f",
            "2442c32974784ece88322b0c107ddf10",
            "c6ef2818f95249a59e75c9bbadee0f5c",
            "9ea7ed3625b74c5d83308309e8358feb",
            "210a1f02a58f4e948c5ad424e258a9e0",
            "5ad89348d26c4c7185f5358f45b7a04f"
          ]
        },
        "id": "s1rG8BllnjL3",
        "outputId": "7d2d48e2-7562-4f29-d888-6f7bb4135546"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Lancelottery/healthy-cxr-race-model is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    197\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    198\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    199\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    200\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[39m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[39m.\u001b[39mproxy\u001b[39m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[39m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1097\u001b[0m \u001b[39m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[0;32m--> 615\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    616\u001b[0m server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    206\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\u001b[39m}\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    208\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x7fe921176890>, 'Connection to huggingface.co timed out. (connect timeout=10)')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Lancelottery/healthy-cxr-race-model/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fe921176890>, 'Connection to huggingface.co timed out. (connect timeout=10)'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(url\u001b[39m=\u001b[39;49murl, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49metag_timeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m   1723\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1646\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1647\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1648\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1649\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1650\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1651\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1652\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1653\u001b[0m )\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    373\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    374\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    375\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    376\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    379\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:395\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    396\u001b[0m hf_raise_for_status(response)\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/requests/adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 688\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    690\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ResponseError):\n",
            "\u001b[0;31mConnectTimeout\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Lancelottery/healthy-cxr-race-model/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fe921176890>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 9ed10f13-2add-458c-82dd-f769bb4ca983)')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    403\u001b[0m         path_or_repo_id,\n\u001b[1;32m    404\u001b[0m         filename,\n\u001b[1;32m    405\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    406\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    407\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    408\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    409\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    410\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    411\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    412\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    413\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    414\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1222\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1224\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   1226\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1227\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   1228\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1229\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1231\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1232\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1233\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m   1234\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1236\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1237\u001b[0m     )\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[39m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1327\u001b[0m \u001b[39m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1826\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1826\u001b[0m     \u001b[39mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1827\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1828\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1829\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1830\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mhead_call_error\u001b[39;00m\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m/root/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/32patch_ViT_Prisma_Main_Demo.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://icoding%2B541752.icoding.baidu-int.com/root/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/32patch_ViT_Prisma_Main_Demo.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We'll use a vanilla vision transformer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://icoding%2B541752.icoding.baidu-int.com/root/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/32patch_ViT_Prisma_Main_Demo.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_prisma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_vit\u001b[39;00m \u001b[39mimport\u001b[39;00m HookedViT\n\u001b[0;32m----> <a href='vscode-notebook-cell://icoding%2B541752.icoding.baidu-int.com/root/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/32patch_ViT_Prisma_Main_Demo.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m HookedViT\u001b[39m.\u001b[39;49mfrom_pretrained(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLancelottery/healthy-cxr-race-model\u001b[39;49m\u001b[39m\"\u001b[39;49m, is_timm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://icoding%2B541752.icoding.baidu-int.com/root/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/32patch_ViT_Prisma_Main_Demo.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# model = torch.load('../weight/32_patched_hooked_vit_model.pth')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://icoding%2B541752.icoding.baidu-int.com/root/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/32patch_ViT_Prisma_Main_Demo.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# model = torch.load('/Users/Lancelot/Downloads/hooked-race-vit.pth')\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/../src/vit_prisma/models/base_vit.py:723\u001b[0m, in \u001b[0;36mHookedViT.from_pretrained\u001b[0;34m(cls, model_name, is_timm, is_clip, fold_ln, center_writing_weights, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, use_attn_result, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    718\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfloat16 models may not work on CPU. Consider using a GPU or bfloat16.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[39m# Set up other parts of transformer\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m cfg \u001b[39m=\u001b[39m convert_pretrained_model_config(\n\u001b[1;32m    724\u001b[0m     model_name,\n\u001b[1;32m    725\u001b[0m     is_timm\u001b[39m=\u001b[39;49mis_timm,\n\u001b[1;32m    726\u001b[0m     is_clip\u001b[39m=\u001b[39;49mis_clip,\n\u001b[1;32m    727\u001b[0m )\n\u001b[1;32m    731\u001b[0m state_dict \u001b[39m=\u001b[39m get_pretrained_state_dict(\n\u001b[1;32m    732\u001b[0m     model_name, is_timm, is_clip, cfg, hf_model, dtype\u001b[39m=\u001b[39mdtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfrom_pretrained_kwargs\n\u001b[1;32m    733\u001b[0m )\n\u001b[1;32m    735\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(cfg, move_to_device\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/demos/../src/vit_prisma/prisma_tools/loading_from_pretrained.py:406\u001b[0m, in \u001b[0;36mconvert_pretrained_model_config\u001b[0;34m(model_name, is_timm, is_clip)\u001b[0m\n\u001b[1;32m    404\u001b[0m         hf_config\u001b[39m.\u001b[39mnum_classes \u001b[39m=\u001b[39m hf_config\u001b[39m.\u001b[39mprojection_dim \u001b[39m# final output dimension instead of classes\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m         hf_config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m    408\u001b[0m \u001b[39m#     print('hf config', hf_config)\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(hf_config, \u001b[39m'\u001b[39m\u001b[39mpatch_size\u001b[39m\u001b[39m'\u001b[39m):\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:965\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    963\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 965\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    966\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    967\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME) \u001b[39mif\u001b[39;00m gguf_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    691\u001b[0m         configuration_file,\n\u001b[1;32m    692\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    693\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    694\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    695\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    696\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    697\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    698\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    699\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    700\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    701\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
            "File \u001b[0;32m~/paddlejob/workspace/env_run/luyanan05/fair-vit/venv/lib/python3.11/site-packages/transformers/utils/hub.py:445\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    440\u001b[0m         resolved_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    441\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries\n\u001b[1;32m    442\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_connection_errors\n\u001b[1;32m    443\u001b[0m     ):\n\u001b[1;32m    444\u001b[0m         \u001b[39mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this file, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory containing a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
            "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Lancelottery/healthy-cxr-race-model is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
          ]
        }
      ],
      "source": [
        "# We'll use a vanilla vision transformer\n",
        "from vit_prisma.models.base_vit import HookedViT\n",
        "# model = HookedViT.from_pretrained(model_name=\"Lancelottery/cxr-race-patch32\", is_timm=False)\n",
        "model = torch.load('../weight/32_patched_hooked_vit_model.pth')\n",
        "# model = torch.load('/Users/Lancelot/Downloads/hooked-race-vit.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEKVJZAlnIhs"
      },
      "source": [
        "We can see above the specs of our model.\n",
        "\n",
        "n_layers: 12\n",
        "\n",
        "d_model: 768\n",
        "\n",
        "d_head: 64\n",
        "\n",
        "n_heads: 12 (per layer)\n",
        "\n",
        "d_mlp: 3072\n",
        "\n",
        "patch_size: 32\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvcfAlnrp9od"
      },
      "source": [
        "## Load data\n",
        "\n",
        "For this specific transformer, ImageNet's 1000 classes serve as a rough equivalent to a language dictionary, but it's important to remember that these classes don't fully encompass all visual concepts, unlike more expansive models like CLIP.\n",
        "\n",
        "<details> <summary>On the ambiguities of vision </summary>\n",
        "\n",
        "Vision introduces unique ambiguities not found in language, including hierarchical ambiguity (e.g., identifying a black dog paw at varying levels of specificity), cultural ambiguity (where different cultures may label images differently), and precision ambiguity (where terms like \"object\" can refer to a wide range of items). While exploring language's ambiguity is beyond this notebook's focus, it's a recurring team in non-language interpretability.\n",
        "\n",
        "</details>\n",
        "\n",
        "For now, we will keep things simple and just deal with 1000-way ImageNet. Let's load our image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "iOwg905fnOwJ",
        "outputId": "ecbcfbc7-d0c2-4640-e96e-10f50c1a624a"
      },
      "outputs": [],
      "source": [
        "image = Image.open('../assets/test_images/white-cxr.jpg')\n",
        "image = transform(image)\n",
        "plot_image(image)\n",
        "#true label = WHITE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajLnFCDhU2v"
      },
      "source": [
        "We'll use an image with both a Border Collie and a tabby cat. I'm curious how the classifier represents each animal throughout the net and 'decides' between the two animals for its final classification.\n",
        "\n",
        "For the sake of this demo, we'll just use one image. However, a more comprehensive analysis would use several images instead of one.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wupq4pryZ9Q"
      },
      "source": [
        "We divide our 224 x 224 image into 32 x 32 non-overlapping patches to feed into the transformer. The transformer also appends a CLS token at the beginning, which is not represented spatially. In total, including the CLS token, there are 50 patches for the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "_enziNCHuBfu",
        "outputId": "f5b913ce-0f0c-485e-b79c-19cb4ecfc36a"
      },
      "outputs": [],
      "source": [
        "from vit_prisma.visualization.visualize_image import display_grid_on_image\n",
        "\n",
        "display_grid_on_image(image, patch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data in Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class ConvertTo3Channels:\n",
        "    def __call__(self, img):\n",
        "        if img.mode != 'RGB':\n",
        "            return img.convert('RGB')\n",
        "        return img\n",
        "\n",
        "# Define your transformations, including the custom ConvertTo3Channels\n",
        "transform = transforms.Compose([\n",
        "    ConvertTo3Channels(),  # Ensure all images are 3-channel without turning them grayscale\n",
        "    transforms.Resize((224, 224)),  # Resize images to a common size.\n",
        "    transforms.ToTensor(),  # Convert images to tensor.\n",
        "    # You can include normalization if desired, using correct values for 3-channel images.\n",
        "])\n",
        "\n",
        "\n",
        "def transform_batch(examples):\n",
        "    images = [transform(image) for image in examples['image']]\n",
        "    label_map = {'WHITE': 0, 'BLACK/AFRICAN AMERICAN': 1, 'ASIAN':2}  \n",
        "    labels = [label_map[race] for race in examples['race']]\n",
        "    labels = torch.tensor(labels, dtype=torch.long) \n",
        "\n",
        "    return {'image': images, 'label': labels}\n",
        "\n",
        "def get_attn_across_datapoints(attn_head_idx, attention_type=\"attn_scores\"):\n",
        "\n",
        "  list_of_attn_heads = [attn_head_idx]\n",
        "\n",
        "  # Retrieve the activations from each batch idx\n",
        "  all_patterns = []\n",
        "  for batch_idx in range(images.shape[0]):\n",
        "    patterns = visualize_attention(list_of_attn_heads, cache, \"Attention Scores\", 700, batch_idx = batch_idx, attention_type=attention_type)\n",
        "    all_patterns.extend(patterns)\n",
        "  all_patterns = torch.stack(all_patterns, dim=0)\n",
        "  return all_patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Set global seeds\n",
        "def set_seed(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
        "    np.random.seed(seed_value)  # Numpy module.\n",
        "    random.seed(seed_value)  # Python random module.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_value = 42\n",
        "set_seed(seed_value)\n",
        "\n",
        "# Define a worker init function that sets the seed\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import DatasetDict\n",
        "# dataset = DatasetDict.load_from_disk(\"../hf_dataset\")\n",
        "\n",
        "# dataset.set_transform(transform_batch)\n",
        "\n",
        "# batch_size = 16\n",
        "# train_loader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=True, num_workers=0, worker_init_fn=worker_init_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQDe8LdjuXI_"
      },
      "source": [
        "## Test prompt\n",
        "\n",
        "Let's feed the image into the transformer and investigate its top logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JesDn5qCuYAk"
      },
      "outputs": [],
      "source": [
        "from vit_prisma.utils.data_utils.race_utils import race_index_from_word\n",
        "from vit_prisma.utils.data_utils.race_dict import RACE_DICT\n",
        "from vit_prisma.utils.prisma_utils import test_prompt\n",
        "test_prompt(image,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(image.shape,image.unsqueeze(0).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-TPJHZjy3fP"
      },
      "source": [
        "We can see the net predicts WHITE as the most probable class. Let's run the model again and cache all the intermediate activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByDwBXKHwTIv"
      },
      "outputs": [],
      "source": [
        "# Run Cache\n",
        "logits, cache = model.run_with_cache(image.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxIKmzx8nQOJ"
      },
      "source": [
        "# Logit Attribution\n",
        "\n",
        "We want to see how each internal component of the model is impacting its output. Direct logit attribution is a powerful technique to do so.\n",
        "\n",
        "<details> <summary>Background</summary>\n",
        "\n",
        "Compared to past architectures like CNNs and RNNs, transformers are unusually interpretable. This is partially because transformers are highly linear, which means we can easily decompose the output of each layer into the contributions of its previous parts.\n",
        "\n",
        "The residual stream of the transformer is the cumulative sum of the outputs of the previous layers, plus the original patch and position embedding.\n",
        "\n",
        "The output of each attention layer can be decomposed into a sum of the attention heads; and the output of each MLP layer can be decomposed into the sum of each neuron.\n",
        "\n",
        "The final logits of the model are `logits=ClassificationHead(LayerNorm(final_residual_stream)`. The classification head is a linear layer, and the LayerNorm is *approximately* a linear layer. Thus, we can decompose the logits into the sum of components, and see which components contribute most to the logit of the correct answer.\n",
        "\n",
        "For example, the tabby cat logit has an index of 281 in our 1000-logit classification head. We can decompose the transformer to see which components of the transformer (MLP layers, attention heads, etc) contribute most strongly to the tabby cat logit.\n",
        "\n",
        "For details on the linear structure of transformer, check out [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html).\n",
        "\n",
        "For more details on direct logit attribution, check out the mech interp explainer [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ) and the original Exploratory Analysis notebook [here](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=tGz70kpdiPzs).\n",
        "\n",
        "<details> <summary> Folding LayerNorm </summary>\n",
        "\n",
        "The original LayerNorm implementation, which is a center and scale, messes with the linearity described above. The LayerNorm is almost a linear operation except for the scaling factor; but thankfully, if we fix the scale factor, then it becomes a global property. Thus, we can create an approximation of the LayerNorm that factors out the linear parts with 'fold_ln' when we load the model, and we can move the scaling factor to the end of the residual stream as a global computation with 'apply_ln'.\n",
        "For more information, check out the [original rationale](https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md) on TransformerLens.\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tffA0RFnq2ow"
      },
      "source": [
        "Let's snag the ImageNet indexes for tabby and border collie.\n",
        "\n",
        "<details> <summary> An aside on superclasses </summary>\n",
        "It's possible to run our analysis with superclasses (e.g. cats vs dogs), whose coarser granularity may more readily reveal properties of the model. However, we'll leave that for a future exercise and focus on these two specific classes.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNIt2u66nsij",
        "outputId": "a66c4fe4-338c-4390-dd2d-19ecc154a6f8"
      },
      "outputs": [],
      "source": [
        "## Accumulated residual\n",
        "white_index = race_index_from_word(\"WHITE\")\n",
        "black_index = race_index_from_word(\"BLACK/AFRICAN AMERICAN\")\n",
        "\n",
        "print(\"White index:\", white_index)\n",
        "print(\"Black index: \", black_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eccOgoVrLcz"
      },
      "source": [
        "We want to compare the logits between WHITE and BLACK.\n",
        "\n",
        "Let's get the residual directions of these two classes.\n",
        "\n",
        "The residual directions are simply indexing into the classication head, which is represented as a 768 x 3 matrix. We take the columns that correspond with the WHITE and BLACK indices. We're taking these size 768 vectors as our residual directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li2uVIrieXx3",
        "outputId": "98d11ebb-a7b6-43f6-e67a-7afd963dcb0a"
      },
      "outputs": [],
      "source": [
        "white_answer_residual_direction = model.tokens_to_residual_directions(white_index)\n",
        "print(\"Size of White residual direction\", white_answer_residual_direction.shape)\n",
        "\n",
        "black_answer_residual_direction = model.tokens_to_residual_directions(black_index)\n",
        "print(\"Size of Black residual direction\", black_answer_residual_direction.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZy4bHKGqgrS"
      },
      "source": [
        "To check that our residual directions are correct, we can dot product them with the model's final residual stream. The answer should equal the raw logit value if we indexed directly into the model's output logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DlbFYece3Ta",
        "outputId": "8f95aa47-b32e-4e3e-cd62-87cd94035b28"
      },
      "outputs": [],
      "source": [
        "# Confirm manually\n",
        "final_residual_stream = cache[\"resid_post\", -1]\n",
        "\n",
        "# Put 0 to get CLS token at beginning\n",
        "cls_token_residual_stream = final_residual_stream[:, 0, :]\n",
        "\n",
        "scaled_final_token_residual_stream = cache.apply_ln_to_stack(cls_token_residual_stream, layer=-1, pos_slice=0 )\n",
        "\n",
        "white_logit = einsum(\n",
        "    \"batch d_model, d_model -> batch\",\n",
        "    scaled_final_token_residual_stream,\n",
        "    white_answer_residual_direction,\n",
        ")\n",
        "\n",
        "black_logit = einsum(\n",
        "    \"batch d_model, d_model -> batch\",\n",
        "    scaled_final_token_residual_stream,\n",
        "    black_answer_residual_direction,\n",
        ")\n",
        "\n",
        "white_logit = white_logit + model.head.b_H[white_index] # Add bias of head\n",
        "print(\"White logit shape: \", white_logit.shape)\n",
        "print(\"Calculated white logit value:\", round(white_logit.item(), 3))\n",
        "print(\"White logit value taken directly from model's output logits:\", round(logits[:,white_index].item(),3))\n",
        "print()\n",
        "\n",
        "black_logit = black_logit + model.head.b_H[black_index]\n",
        "print(\"Black logit shape: \", black_logit.shape)\n",
        "print(\"Calculated Black logit value:\", round(black_logit.item(), 3))\n",
        "print(\"Black logit value taken directly from model's output logits:\", round(logits[:,black_index].item(), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcpzOTx7rhm0"
      },
      "source": [
        "## All Logit Values\n",
        "\n",
        "We can project every intermediate CLS token output from each layer onto the residual directions. This is the equivalent of directly feeding the CLS output of each layer into the 3-way classification head to get a logit value for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLq0roLewx3W"
      },
      "outputs": [],
      "source": [
        "def average_logit_value_across_all_classes(\n",
        "    residual_stack,\n",
        "    cache,\n",
        "    mean = True,\n",
        "):\n",
        "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
        "            residual_stack, layer=-1, pos_slice=0\n",
        "        )\n",
        "\n",
        "    all_residual_directions = model.tokens_to_residual_directions(np.arange(3)) # Get all residual directions\n",
        "\n",
        "    print(f\"scaled_residual_stack shape: {scaled_residual_stack.shape}\")\n",
        "    print(f\"all_residual_directions shape: {all_residual_directions.shape}\")\n",
        "\n",
        "    logit_predictions = einsum(\n",
        "        \"layer batch d_model, batch d_model -> batch layer\",\n",
        "        scaled_residual_stack,\n",
        "        all_residual_directions,\n",
        "    )\n",
        "    if mean:\n",
        "      logit_predictions = logit_predictions.mean(axis=0)\n",
        "\n",
        "    return logit_predictions\n",
        "\n",
        "accumulated_residual, layer_labels = cache.accumulated_resid(\n",
        "        layer=-1, incl_mid=True, pos_slice=0, return_labels=True\n",
        "    )\n",
        "average_logits = average_logit_value_across_all_classes(accumulated_residual, cache, mean=False)\n",
        "\n",
        "print(f\"average_logits shape: {average_logits.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNQYQf2NuMIj"
      },
      "source": [
        "\n",
        "We can see that the final attention layer most significantly changes the distribution of logits! The plot is interactive; hover your mouse over the outliers to see the class names.z\n",
        "\n",
        "**Key for graph below**\n",
        "* n_pre - residual stream at start of layer n\n",
        "* n_mid - residual stream after attention part of layer\n",
        "* final_post - residual stream after final mlp layer (before classification head)\n",
        "\n",
        "<details>\n",
        "<summary><i>Note on raw logit metric for more experienced mech interp people</i></summary>\n",
        "\n",
        "The logit difference between two classes, rather than the raw logit value itself, is often considered a more reliable measure. This is because when subtracting one logit from another, any arbitrary constants inherent in the logits cancel out. The design of cross-entropy loss focuses specifically on the differences between class logits, which can result in the logits being adjusted by an arbitrary constant. For a more in-depth explanation, refer to the mechanism interpretation explainer [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ) or the original Exploratory Analysis notebook [here](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=tGz70kpdiPzs).\n",
        "\n",
        "Contrary to the initial Indirect Object Identification (IOI) analysis, which focuses on the logit differences between two classes, my approach considers the raw logit value of a single class. Observing a box plot of all logit values reveals that, on average, the values are near zero for every layer (indicated by the red dot). Therefore, the notion of biases neutralizing each other, as seen in the logit difference case, does not hold here since the average logit value is zero.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "kWhOyNstAGqD",
        "outputId": "d42be01f-15d0-4158-9e23-915c548c14c5"
      },
      "outputs": [],
      "source": [
        "plot_logit_boxplot(average_logits, layer_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "average_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmLYxKo1ujjj"
      },
      "source": [
        "### Layer-Level Logit lens\n",
        "We'll now project every layer of the image's residual stream onto the residual directions. We'll use just the CLS token for this analysis.\n",
        "\n",
        "This is analogous to deleting all subsequent layers and just projecting that layer's output into the classification head. This will tell us the likelihood the net assigns to each class throughout the net.\n",
        "\n",
        "Let's also grab a random logit, like banana, for comparison. We'll also get the average of all logits as a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IutGakbRk_3s",
        "outputId": "843ced38-3ea3-4cd1-fdcf-678c7d4f718e"
      },
      "outputs": [],
      "source": [
        "# misc_index = imagenet_index_from_word(\"banana\")\n",
        "# print(\"Banana index:\", misc_index)\n",
        "# I chose Asian here\n",
        "misc_residual_direction = model.tokens_to_residual_directions(2)\n",
        "print(\"Asian index:\", 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqPo5RSDsrp-"
      },
      "source": [
        "\n",
        "Let's plot the logit values for WHITE, BLACK, and ASIAN throughout the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgxmWoF_GZjI"
      },
      "outputs": [],
      "source": [
        "def residual_stack_to_logit(\n",
        "    residual_stack,\n",
        "    cache,\n",
        "    answer_residual_directions\n",
        "):\n",
        "\n",
        "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
        "            residual_stack, layer=-1, pos_slice=0\n",
        "        )\n",
        "        \n",
        "    #scaled_residual_stack shape: [25, 1, 768]\n",
        "    #answer_residual_directions shape: [768]\n",
        "\n",
        "    logit_predictions = einsum(\n",
        "        \"layer batch d_model, d_model -> layer\",\n",
        "        scaled_residual_stack,\n",
        "        answer_residual_directions,\n",
        "    )\n",
        "    print(\"logit_predictions after einsum:\", logit_predictions.shape)\n",
        "    return logit_predictions\n",
        "\n",
        "accumulated_residual, layer_labels = cache.accumulated_resid(\n",
        "        layer=-1, incl_mid=True, pos_slice=0, return_labels=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAeKsmZfwQb3"
      },
      "source": [
        "The plot below is interactive. Run your cursor over the layers to see the corresponding logit value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "dWo3Dk8yoh9U",
        "outputId": "3637cbfb-79e6-4da6-9e9c-995f9d6b2f42"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "residual_directions = [\n",
        "    (white_answer_residual_direction, 'White Logit', model.head.b_H[0], 'dark blue'), # blue for white\n",
        "    (black_answer_residual_direction, 'Black Logit', model.head.b_H[1], 'black'), \n",
        "    (misc_residual_direction, 'Asian (non-relevant) Logit', model.head.b_H[2], 'yellow'), \n",
        "]\n",
        "\n",
        "hover_text = [f\"{layer_labels[idx]}\" for idx, diff in enumerate(accumulated_residual)]\n",
        "\n",
        "# Process each residual direction\n",
        "for direction, label, bias, color in residual_directions:\n",
        "    logit_predictions = residual_stack_to_logit(accumulated_residual, cache, direction)\n",
        "    logit_predictions = logit_predictions.T + bias  # Add bias\n",
        "    logit_predictions = logit_predictions.detach().numpy()\n",
        "    fig.add_trace(go.Scatter(x= np.arange(model.cfg.n_layers * 2 + 1) / 2, y=logit_predictions, mode='lines', name=label, text=hover_text, line=dict(color=color)))\n",
        "\n",
        "# Handle \"Average Logit Across All Classes\" separately\n",
        "average_total_logits = average_logit_value_across_all_classes(accumulated_residual, cache).detach().numpy()\n",
        "print(average_total_logits.shape)\n",
        "fig.add_trace(go.Scatter(x= np.arange(model.cfg.n_layers * 2 + 1) / 2, y=average_total_logits, mode='lines', name='Average Logit Across All Classes'))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Logit through Model Layers\",\n",
        "    xaxis_title=\"Model Layer\",\n",
        "    yaxis_title=\"Logit Value\",\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqcYNIUlsOV7"
      },
      "source": [
        "#NOTE should try with different & unrelevant classes\n",
        "\n",
        "\n",
        "We can see the model doesn't really weight the WHITE prediction too highly until Layer 10! The prediction is around equal to the ASIAN prediction. Interestingly, it takes until Layer 11 for the model to decide that the image is *not* an ASIAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "4q7Cm4vAlt6N",
        "outputId": "05214477-2b7b-47b8-bdbb-ba7aba7b0fa8"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Assuming `cache`, `average_logit_value_across_all_classes`, `residual_stack_to_logit`, and other necessary functions and variables are defined elsewhere\n",
        "\n",
        "# Retrieve the per-layer residual and labels\n",
        "per_layer_residual, layer_labels = cache.decompose_resid(\n",
        "    layer=-1, pos_slice=0, return_labels=True\n",
        ")\n",
        "\n",
        "# Initialize the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Iterate over each residual direction and its corresponding label\n",
        "for direction, label, bias, color in residual_directions:\n",
        "    per_layer_logit_diffs = residual_stack_to_logit(per_layer_residual, cache, direction)\n",
        "    per_layer_logit_diffs = per_layer_logit_diffs.detach().numpy()  # Assuming you want the first element for some reason; adjust if necessary\n",
        "    hover_text = [f\"{layer_labels[idx]}\" for idx, diff in enumerate(per_layer_logit_diffs)]\n",
        "    fig.add_trace(go.Scatter(y=per_layer_logit_diffs, mode='lines', name=label, text=hover_text, line=dict(color=color)))\n",
        "\n",
        "# Handle average logits across all classes\n",
        "average_total_logits = average_logit_value_across_all_classes(per_layer_residual, cache).detach().numpy()\n",
        "fig.add_trace(go.Scatter(y=average_total_logits, name=\"Average Logit Across All Labels\"))\n",
        "\n",
        "# Update layout with appropriate titles\n",
        "fig.update_layout(\n",
        "    title=\"Logit through Model Layers\",\n",
        "    xaxis_title=\"Layer\",\n",
        "    yaxis_title=\"Logit Difference\",\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM41pXX7xeUG"
      },
      "source": [
        "We can do the same analysis above, but for each layer. This is equivalent to taking the difference between each adjacent residual stream.\n",
        "\n",
        "We can see that the final attention layer does the most work in deciding the computation. The subsequent MLP layer actually makes its performance worse!\n",
        "\n",
        "The last two layers seem to being the bulk of the work in identifying the WHITE and the BLACK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCa1RPyFoi7v"
      },
      "source": [
        "## Patch-level logit lens\n",
        "\n",
        "We ran the analysis above on just the CLS token of the model. But we actually have 49 other patches that we haven't investigated yet.\n",
        "\n",
        "(As a reminder, our 224 x 224 image was divided into 50 non-overlapping patches. Each patch is 32 x 32. The patches are analogous to tokens in an LLM.)\n",
        "\n",
        "The attention layers move information between patches, while the MLP layers process information within each patch. One open question in vision mech interp is what each patch represents throughout the net, and how information moves between the patches. How is information \"gathered\" across a cat whisker, cat ear, and cat eye to lead to a cat classification? Does every patch contain local information from the original image, even in the last layers of the net? For instance, does a patch initially containing a dog ear retain dog ear information, even after 12 layers of global self-attention? Or does it start storing higher abstractions, such as, dog face information, and dog breed information?\n",
        "\n",
        "A patch-level logit lens is a good starting point to investigate these questions. The patch-level logit lens is the same as the analysis done above, but run on every patch instead of just the CLS token.\n",
        "\n",
        "We'll soon see that each patch encodes information (mostly) locally, to the extent that we can use the patch-level logit lens as a segmentation map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ux63_tX1bM0"
      },
      "outputs": [],
      "source": [
        "from vit_prisma.prisma_tools.race_logit_lens import get_patch_logit_directions, get_patch_logit_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exweeIxKoAAF",
        "outputId": "f8621c6f-e150-44cb-b661-df0349ee3628"
      },
      "outputs": [],
      "source": [
        "# Can also import the above as functions; displayed in full above for didactic purposes.\n",
        "# from vit_prisma.prisma_tools.logit_lens import get_patch_logit_directions, get_patch_logit_dictionary\n",
        "\n",
        "all_answers = model.tokens_to_residual_directions(np.arange(2))\n",
        "patch_logit_directions = get_patch_logit_directions(cache, all_answers, incl_mid=True)\n",
        "\n",
        "patch_dictionary = get_patch_logit_dictionary(patch_logit_directions, rank_label='white')\n",
        "print(\"Length of patch dictionary:\", len(patch_dictionary)) # Number of patches\n",
        "print(\"Length of one dictionary entry:\", len(list(patch_dictionary.values())[0])) # Layers\n",
        "\n",
        "white_rank_formatted = np.array([[item[-1] for item in list(patch_dictionary.values())[i]] for i in range(50)])\n",
        "\n",
        "patch_dictionary = get_patch_logit_dictionary(patch_logit_directions, rank_label='black')\n",
        "black_rank_formatted = np.array([[item[-1] for item in list(patch_dictionary.values())[i]] for i in range(50)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patch_logit_directions[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr_n8KHizpSF"
      },
      "source": [
        "We'll see how highly each layer ranks WHITE and BLACK in its predictions to get a rank for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "5QJGb42PnWbB",
        "outputId": "c3879ed6-9fff-4445-ddd7-159f7703d8bc"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "import numpy as np\n",
        "\n",
        "# Initialize traces list for Plotly\n",
        "traces = []\n",
        "\n",
        "# Define colors with darker shades\n",
        "colors = {\n",
        "    'white': {'cls_color': 'rgba(0, 0, 255, 1.0)', 'patch_color': 'rgba(169, 169, 169, 0.5)'},\n",
        "    'black': {'cls_color': 'rgba(255, 0, 0, 1.0)', 'patch_color': 'rgba(218, 165, 32, 0.5)'},\n",
        "}\n",
        "\n",
        "# Data setup for each class\n",
        "data_classes = [\n",
        "    {'name': 'white', 'data': white_rank_formatted},\n",
        "    {'name': 'black', 'data': black_rank_formatted},\n",
        "]\n",
        "\n",
        "# Processing both classes in a single loop\n",
        "for class_info in data_classes:\n",
        "    rank_data = class_info['data']\n",
        "    class_name = class_info['name']\n",
        "    cls_color = colors[class_name]['cls_color']\n",
        "    patch_color = colors[class_name]['patch_color']\n",
        "\n",
        "    for i, patch_data in enumerate(rank_data):\n",
        "        color = cls_color if i == 0 else patch_color\n",
        "        width = 2 if i == 0 else 1\n",
        "        mode = 'lines+markers' if i == 0 else 'lines'\n",
        "        trace = go.Scatter(\n",
        "            x=np.arange(len(patch_data) * 2 + 1) / 2,\n",
        "            y=patch_data,\n",
        "            mode=mode,\n",
        "            line=dict(color=color, width=width),\n",
        "            hoverinfo='text',\n",
        "            text=[f'Layer {x + 1}, Patch {i} - {class_name.capitalize()}, Rank: {y}' for x, y in enumerate(patch_data)],\n",
        "            name=f'{class_name.capitalize()} Rank for CLS' if i == 0 else f'{class_name.capitalize()} Rank',\n",
        "            showlegend=i < 2  # Show legend only for the first CLS and patch\n",
        "        )\n",
        "        traces.append(trace)\n",
        "\n",
        "# Plotting configuration\n",
        "fig = go.Figure(traces)\n",
        "fig.update_layout(\n",
        "    title='Patch Ranks Over Layers for White and Black',\n",
        "    xaxis_title='Layer',\n",
        "    yaxis_title='Rank',\n",
        "    hovermode='closest',\n",
        "    legend_title='Legend',\n",
        "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IokRulR_JAT"
      },
      "source": [
        "Above, we've done a logit lens on every individual patch. We've plotted each layer's ranking for the WHITE and BLACK classes.\n",
        "\n",
        "The patch info appears to have a decent amount of spatial localization. The patches for which both cat/collie rank poorly (patches 15, 21, 7, 35) correspond to the edges of the image. Patch 25 ranks poorly for tabby, but well for dog, which spatially corresponds to the dog.\n",
        "\n",
        "It's interesting that the rank for the cat and collie somewhat trade off against each other in the middle of the net (Layers 4-7).\n",
        "\n",
        "As a sanity check, let's graph the rank of the CLS token alongside its logit value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "white_rank_formatted.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "PEr1a2jQleq9",
        "outputId": "cc916c26-c897-453f-faa8-10e0be078158"
      },
      "outputs": [],
      "source": [
        "# Graph CLS rank and logit together\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "cls_token = white_rank_formatted[0]\n",
        "\n",
        "# Create a figure with secondary y-axis\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "# Add rank plot\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(len(cls_token))), y=cls_token, name=\"Rank of white (CLS token)\", mode='lines+markers'),\n",
        "    secondary_y=False,\n",
        ")\n",
        "\n",
        "# Get cat residual direction independently\n",
        "white_logit_predictions = residual_stack_to_logit(accumulated_residual, cache, white_answer_residual_direction)\n",
        "white_logit_predictions = white_logit_predictions.T + bias  # Add bias\n",
        "white_logit_predictions = white_logit_predictions.detach().numpy()\n",
        "\n",
        "\n",
        "# Add logit difference plot\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(len(white_logit_predictions))), y=white_logit_predictions, name=\"Logit diff\", mode='lines+markers'),\n",
        "    secondary_y=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Add figure title\n",
        "fig.update_layout(\n",
        "    title_text=\"Rank of White and Logit Differences Through Model Layers\"\n",
        ")\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"Model Layers\")\n",
        "fig.update_yaxes(title_text=\"<b>Rank</b> of tabby cat\", secondary_y=False)\n",
        "fig.update_yaxes(title_text=\"<b>Logit Difference</b>\", secondary_y=True)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYLT8JWkz8MP"
      },
      "source": [
        "### Emoji logit lens\n",
        "It'd be nice to have a clearer visual representation of what is happening. Let's get the predicted value for each patch for every layer of the net. This is equivalent to passing every layer's output into the classification head and seeing the prediction.\n",
        "\n",
        "Hover over the emoji logit lens below to see the class name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lydOfJhm0g-W",
        "outputId": "dee13f39-38c8-4aa1-fd59-b606b576dbe7"
      },
      "outputs": [],
      "source": [
        "from vit_prisma.utils.data_utils.race_emoji import RACE_EMOJI\n",
        "from vit_prisma.visualization.race_patch_level_logit_lens import display_patch_logit_lens\n",
        "\n",
        "\n",
        "patch_logit_directions = get_patch_logit_directions(cache, all_answers, incl_mid=False)\n",
        "\n",
        "patch_dictionary = get_patch_logit_dictionary(patch_logit_directions, batch_idx=0)\n",
        "\n",
        "display_patch_logit_lens(patch_dictionary, labels=layer_labels[::2], width=1300, height=1000, emoji_size=22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patch_dictionary[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "584STXbgGdgH"
      },
      "source": [
        "### WHITE/ BLACK patch-level map\n",
        "\n",
        "We can overlay the final layer predictions of the patch-level logit lens on top of the original image.\n",
        "\n",
        "Also, some patches (yellow) seem to contribute more to the logits than others. Perhaps these patches have more predictive \"white\" or \"black\" information.\n",
        "\n",
        "As for the random-seeming predictions like \"aircraft carrier\" on patch 7; it's not clear why the net is predicting this. Reverse-engineering why it predicts aircraft carrier is a satisfying future research direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "wiWe-wtjHUPS",
        "outputId": "dcd034a6-676f-421d-efa7-d885c07cfdd1"
      },
      "outputs": [],
      "source": [
        "from vit_prisma.visualization.race_patch_level_logit_lens import display_grid_on_image_with_heatmap\n",
        "fig = display_grid_on_image_with_heatmap(image, patch_dictionary, alpha_color=.4, emoji_font_size=10, return_graph=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws6ZkvMT5PCO"
      },
      "source": [
        "The segmentation map becomes even more pronounced if we group all the dogs together and all the cats together and color-code by logit value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "5bWALEsb5R1m",
        "outputId": "4092e598-8a9c-482f-ffaf-fff1976bc90d"
      },
      "outputs": [],
      "source": [
        "fig = display_grid_on_image_with_heatmap(image, patch_dictionary, heatmap_mode='emoji_colors',emoji_alpha=.7, emoji_font_size=10,  return_graph=True, alpha_color=.4, layer_idx=-1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqfqH7xYY03P"
      },
      "source": [
        "It's interesting that the model still predict cat despite there being more dog patches and more dog classes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoaGtkFN37mf"
      },
      "source": [
        "It is fascinating that the model predicts Border collie even for the blank patches on the right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLzm_X_BG7kk"
      },
      "source": [
        "# The Cat - Dog Switch\n",
        "\n",
        "I'm curious why the network predicts WHITE over BLACK. Can we localize the point in the network that makes this \"decision\"?\n",
        "\n",
        "Let's take the *logit difference* between the WHITE and BLACK predictions as our new metric.\n",
        "\n",
        "***Is it possible to switch the network's prediction from white to black?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9NyHS-PHIjq",
        "outputId": "64600be0-d659-4324-94a6-ebf0f44f9fe4"
      },
      "outputs": [],
      "source": [
        "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
        "    # Only the final logits are relevant for the answer\n",
        "    answer_logits = logits[:, answer_tokens]\n",
        "    answer_logits = torch.logsumexp(answer_logits, dim=0) # sum multiple labels if present\n",
        "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
        "    if per_prompt:\n",
        "        return answer_logit_diff\n",
        "    else:\n",
        "        return answer_logit_diff.mean()\n",
        "\n",
        "\n",
        "answer_tokens = [[white_index, black_index]]\n",
        "answer_tokens = torch.Tensor(answer_tokens).long()\n",
        "\n",
        "logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)\n",
        "print(\"The logit difference between White and Black is: \", logit_diff.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz76n_QVH76U"
      },
      "source": [
        "This means that the net is e^-1.56 = 0.21; 1/0.21 = **x4.76** more likely to pick the Border Collie than the tabby cat.\n",
        "A negative value mean the net classifies the image as tabby cat. Positive means the net classifies the image as Border Collie.\n",
        "\n",
        "\n",
        "A positive logit difference indicates that the network predicts the image to be more likely a member of the first class (white) compared to the second class (black).\n",
        "This means that the net is e^0.125 = 1.15; 1/0.867 = **x1.133** more likely to pick the White than the Black.\n",
        "\n",
        "\n",
        "We can graph out the logit difference through the layers below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZgzkvZsJAuL",
        "outputId": "621db106-5493-4835-840e-4c374328399e"
      },
      "outputs": [],
      "source": [
        "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
        "\n",
        "logit_diff_directions = (\n",
        "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
        ")\n",
        "print(\"Logit difference directions shape:\", logit_diff_directions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "YcJ9MDDKIued",
        "outputId": "f18d74ff-c8d9-4094-d290-d252829f8388"
      },
      "outputs": [],
      "source": [
        "def residual_stack_to_logit_diff(\n",
        "    residual_stack,\n",
        "    cache,\n",
        ") -> float:\n",
        "\n",
        "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
        "        residual_stack, layer=-1, pos_slice=0\n",
        "    )\n",
        "    logit_diff = einsum(\n",
        "        \"... batch d_model, batch d_model -> ...\",\n",
        "        scaled_residual_stack,\n",
        "        logit_diff_directions,\n",
        "    )\n",
        "    print(\"logit_diff shape: \", logit_diff.shape)\n",
        "    return logit_diff\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "accumulated_residual, layer_labels = cache.accumulated_resid(\n",
        "        layer=-1, incl_mid=True, pos_slice=0, return_labels=True\n",
        "    )\n",
        "logit_diff = residual_stack_to_logit_diff(accumulated_residual, cache) + model.head.b_H[black_index] - model.head.b_H[white_index]\n",
        "fig.add_trace(go.Scatter(x= np.arange(model.cfg.n_layers * 2 + 1) / 2, y=logit_diff.detach().numpy(), mode='lines', name='Average Logit Across All Classes'))\n",
        "\n",
        "# Add a horizontal line at y=0 in red\n",
        "fig.add_shape(type=\"line\",\n",
        "                x0=0, y0=0, x1=1, y1=0,\n",
        "                line=dict(color=\"Red\", width=2),\n",
        "                xref='paper', yref='y')\n",
        "\n",
        "x_points = np.arange(model.cfg.n_layers * 2 + 1) / 2\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"White - Black Logit Difference through Model Layers\",\n",
        "    xaxis_title=\"Model Layer\",\n",
        "    yaxis_title=\"Logit Difference\",\n",
        "    xaxis=dict(\n",
        "        tickvals=x_points,  # Positions at which ticks should be displayed\n",
        "        ticktext=layer_labels  # Text labels for the ticks\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Ensure the 'x' position for annotations is the last point on the x-axis\n",
        "last_x_position = x_points[-1]\n",
        "\n",
        "# Adjust annotations to reference 'x' and 'y' axes correctly\n",
        "fig.add_annotation(x=last_x_position, y=-0.1, text=\"Black prediction\", showarrow=False, yref=\"y\", xref=\"x\", font=dict(color=\"black\"), align=\"right\")\n",
        "fig.add_annotation(x=last_x_position, y=0.1, text=\"White prediction\", showarrow=False, yref=\"y\", xref=\"x\", font=dict(color=\"black\"), align=\"right\")\n",
        "\n",
        "# Update the 'x1' attribute in the red line shape to span the full width of the x-axis\n",
        "fig.update_shapes(dict(xref='x', x0=0, x1=last_x_position))\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accumulated_residual.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR4Wrj_JJUdE"
      },
      "source": [
        "Can we keep the logit prediction equal? ***Let's find the attention head responsible and disable it, so the prediction stays at Border Collie.***\n",
        "\n",
        "## Head Attribution\n",
        "\n",
        "We can break the output of the attention layer into the sum of outputs of each attention head. See [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) for more details.\n",
        "\n",
        "We can see that the last layer of heads is the most active! Most of the computation happens here, corroborating our plots above.\n",
        "\n",
        "A highly negative logit means that the attention head contributes more to the tabby cat computation. Positive means the attention head contributes more to the dog computation. We have a good mix of the two in the final layer!\n",
        "\n",
        "Head 4 of Layer 11 pushes the prediction toward tabby (negative) while Head 5 of Layer 11 pushes the prediction toward Border Collie (positive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1ZzZO78MiwR"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "\n",
        "def residual_stack_to_logit_attn(\n",
        "    residual_stack,\n",
        "    cache,\n",
        "    answer_residual_direction\n",
        ") -> float:\n",
        "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
        "        residual_stack, layer=-1, pos_slice=0\n",
        "    )\n",
        "    print(scaled_residual_stack.shape)\n",
        "    print(answer_residual_direction.shape)\n",
        "    return einsum(\n",
        "        \"... batch d_model, batch d_model -> ...\",\n",
        "        scaled_residual_stack,\n",
        "        answer_residual_direction,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "ZwOzfN4sMVuc",
        "outputId": "0c1de808-2ea8-4e23-a066-d823ae066380"
      },
      "outputs": [],
      "source": [
        "per_head_residual, labels = cache.stack_head_results(\n",
        "    layer=-1, pos_slice=0, return_labels=True\n",
        ")\n",
        "\n",
        "per_head_logit_diffs = residual_stack_to_logit_attn(per_head_residual, cache, logit_diff_directions)\n",
        "\n",
        "per_head_logit_diffs = einops.rearrange(\n",
        "    per_head_logit_diffs,\n",
        "    \"(layer head_index) -> layer head_index\",\n",
        "    layer=model.cfg.n_layers,\n",
        "    head_index=model.cfg.n_heads,\n",
        ")\n",
        "px.imshow(\n",
        "    per_head_logit_diffs.detach().cpu().numpy(),\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
        "    title=\"Logit Difference From Each Head (CLS token)\",\n",
        "    color_continuous_scale='RdBu_r',  # Use Red-Blue color scale for warm to cool transition\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENZYEbZM2TX"
      },
      "source": [
        "## Interactive attention heads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaYc6PALNV55"
      },
      "source": [
        "\n",
        "Let's examine the attention heads visually to get a better sense of what they're doing.\n",
        "\n",
        "### Plotting Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubpM--XeNTnj"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(\n",
        "    heads,\n",
        "    local_cache,\n",
        "    # local_tokens,\n",
        "    title,\n",
        "    max_width,\n",
        "    batch_idx,\n",
        "    attention_type = 'attn_scores' # or 'attn_patterns'\n",
        ") -> str:\n",
        "    # If a single head is given, convert to a list\n",
        "    if isinstance(heads, int):\n",
        "        heads = [heads]\n",
        "\n",
        "    # Create the plotting data\n",
        "    labels: List[str] = []\n",
        "    patterns: List[Float[torch.Tensor, \"dest_pos src_pos\"]] = []\n",
        "\n",
        "    for head in heads:\n",
        "        # Set the label\n",
        "        layer = head // model.cfg.n_heads\n",
        "        head_index = head % model.cfg.n_heads\n",
        "        labels.append(f\"L{layer}H{head_index}\")\n",
        "\n",
        "        # Get the attention patterns for the head\n",
        "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
        "        patterns.append(local_cache[attention_type, layer][batch_idx, head_index])\n",
        "\n",
        "    # Combine the patterns into a single tensor\n",
        "    patterns = torch.stack(\n",
        "        patterns, dim=0\n",
        "    )\n",
        "    return patterns# Look at those heads [display all heads, modify function if necessary]\n",
        "\n",
        "def print_layer_head_format(tensor_of_heads):\n",
        "  for head in tensor_of_heads:\n",
        "    layer = head // model.cfg.n_heads\n",
        "    head_index = head % model.cfg.n_heads\n",
        "    print(\"Layer: \", layer.item(), \"Head:\", head_index.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XtlLLlRrKOh"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "Let's first visualize the most negative attention heads (which contribute toward BLACK).\n",
        "\n",
        "Each axis of the attention head below is the image flattened (50 patches / squares on the attention head). The y axis is the query, and the x axis is the key.\n",
        "\n",
        "Run your cursor over the visualization below to see the corresponding patches on the image that are correlating with each other. Use the drop down menu of Attention Heads to click between the top three attention heads contributing toward the tabby cat.\n",
        "\n",
        "**Some Observations**\n",
        "\n",
        "* Attention Head 1 - Strongly attends to the cat's face! (pink square)\n",
        "* Attention Head 2 - Strongly attends to a random patch to the left of the dog. This may be a notebook/scratchpad token - an unimportant token in the image that starts accruing global information. See [Do Vision Transformers Need Registers](https://arxiv.org/pdf/2309.16588.pdf) for more informatoin.\n",
        "* Attention Head 3 - Also strongly attends to the random patch to the left of the dog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "YYd7F8yJNv_z",
        "outputId": "f26b0a48-29c1-4600-d0b9-6b3422547003"
      },
      "outputs": [],
      "source": [
        "from vit_prisma.visualization.visualize_attention_js import plot_javascript\n",
        "\n",
        "top_k = 5\n",
        "top_negative_logit_attr_heads = torch.topk(\n",
        "    per_head_logit_diffs.flatten(), k=top_k, largest=False\n",
        ").indices\n",
        "\n",
        "print_layer_head_format(top_negative_logit_attr_heads)\n",
        "\n",
        "print(\"Top negative logit heads (flattened indices):\", top_negative_logit_attr_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_idx = 0\n",
        "scores = visualize_attention(top_negative_logit_attr_heads, cache, \"Attention Scores\", 700, batch_idx = batch_idx, attention_type=\"attn\")\n",
        "html_code = plot_javascript(scores, [image]*top_k, ATTN_SCALING=8)\n",
        "display(HTML(html_code))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNib7bWuy9F6"
      },
      "source": [
        "We can now look at the top postive heads, or the heads that contribute most strongly to the WHITE computation. We can see that the same notebook/scratchpad token to the left of the dog is highly active!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_k = 5\n",
        "top_positive_logit_attr_heads = torch.topk(\n",
        "    per_head_logit_diffs.flatten(), k=top_k\n",
        ").indices\n",
        "\n",
        "print_layer_head_format(top_positive_logit_attr_heads)\n",
        "\n",
        "print(\"Top positive logit heads:\", top_positive_logit_attr_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "5yGUskQdNbt1",
        "outputId": "80728a81-10b8-40a9-c172-0d3240886f1a"
      },
      "outputs": [],
      "source": [
        "batch_idx = 0\n",
        "scores = visualize_attention(top_positive_logit_attr_heads, cache, \"Attention Scores\", 700, batch_idx = batch_idx, attention_type=\"attn\")\n",
        "html_code = plot_javascript(scores, [image]*top_k, ATTN_SCALING=8)\n",
        "display(HTML(html_code))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc9fLio3pOzK"
      },
      "source": [
        "## Activation patching\n",
        "\n",
        "While direct logit attribution gets at the parts of the model that directly impact the logits, there may be significant parts of the model that have an indirect effect. For example, the attention head of an early layer may significantly impact a downstream MLP layer, but its direct impact may not show up with direct logit attribution.\n",
        "\n",
        "Activation patching gives a causal way to see the impact of that component on the logit. The technique was first introduced in [David Bau and Kevin Meng's ROME paper](https://rome.baulab.info/).\n",
        "\n",
        "For our image, we want to see the contribution of every model component on predicting cats vs dogs. We'll crop out the cat to create a \"corrupted\" activation. The goal is to find the parts of the net that are \"deciding\" between cat/dog as the final logit.\n",
        "\n",
        "We'll run the corrupted image through the model and cache the corrupted activations. Then, we will then systematically activation patch in the corrupted activations from the corrupted image into each model component individually. We'll see how much each corrupted activation patch changes the model's original decision.\n",
        "\n",
        "Can we flip the prediction from tabby cat to Border Collie?\n",
        "\n",
        "*Note: Confusingly, the word \"patch\" is being used in two contexts here: activating patching, and the patches/tokens of the original image. I'll put \"activation patch\" before taking about the activation patching here.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "Hb0DwyvZplgR",
        "outputId": "f05507db-01f7-49bd-8174-dc669a15d0b3"
      },
      "outputs": [],
      "source": [
        "clean_image = Image.open('../assets/test_images/white-cxr.jpg')\n",
        "clean_image = transform(clean_image)\n",
        "\n",
        "plot_image(clean_image)\n",
        "corrupted_image = Image.open('../assets/test_images/white-cxr-crop.jpg')\n",
        "corrupted_image = transform(corrupted_image)\n",
        "\n",
        "plot_image(corrupted_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uSSFqTUPgxO",
        "outputId": "a95a7f5a-811c-4fa7-fae9-e67292c3ecb4"
      },
      "outputs": [],
      "source": [
        "answer_tokens = [[white_index, black_index]]\n",
        "answer_tokens = torch.Tensor(answer_tokens).long()\n",
        "\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_image.unsqueeze(0))\n",
        "clean_logit_difference = logits_to_ave_logit_diff(clean_logits, answer_tokens)\n",
        "print(\"Logit difference of clean:\", clean_logit_difference.item())\n",
        "\n",
        "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_image.unsqueeze(0))\n",
        "corrupted_logit_difference = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
        "print(\"Logit difference of corrupted:\", corrupted_logit_difference.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp-AZPWP0lXR"
      },
      "source": [
        "When we put the black box on top of the tabby cat, we can see that the net switches its prediction from tabby cat to Border Collie. Before, the net was  e^-1.56 = 0.21; 1/0.21 = x4.76 more likely to pick tabby cat over Border Collie. Now, the net with the corrupted input is e^5.10 = x164 more likely to pick Border Collie over tabby cat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71O1Zc4z4KkE"
      },
      "source": [
        "## Patching attention heads\n",
        "\n",
        "Let's run a head-by-head activation patching to localize the computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvPDqxdlrSAu"
      },
      "source": [
        "### Plotting code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZoexY9f2LoH"
      },
      "outputs": [],
      "source": [
        "# patch in the heads\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "def patch_head_vector(\n",
        "    original_head_vector,\n",
        "    hook,\n",
        "    head_index,\n",
        "    cache,\n",
        "    patch_num\n",
        "):\n",
        "\n",
        "    original_head_vector[:, patch_num, head_index, :] = cache[hook.name][\n",
        "        :, patch_num, head_index, :\n",
        "    ]\n",
        "    return original_head_vector\n",
        "\n",
        "def run_patch(name): # name can be q, k, v, z\n",
        "    patched_components = torch.zeros(\n",
        "    50, model.cfg.n_heads, dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    for patch_num in tqdm(range(50)):\n",
        "        for head_index in range(model.cfg.n_heads):\n",
        "            hook_fn = partial(patch_head_vector, head_index=head_index, cache=corrupted_cache, patch_num=patch_num)\n",
        "            patched_logits = model.run_with_hooks(\n",
        "                clean_image.unsqueeze(0),\n",
        "                fwd_hooks=[(prisma_utils.get_act_name(name, 11, \"attn\"), hook_fn)],\n",
        "            )\n",
        "            patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens) + model.head.b_H[black_index] - model.head.b_H[white_index]\n",
        "\n",
        "            patched_components[patch_num, head_index] = patched_logit_diff.detach()\n",
        "\n",
        "    return patched_components\n",
        "\n",
        "def run_all_attn_heads(layers=12): # name can be q, k, v, z\n",
        "    patched_heads = torch.zeros(\n",
        "    layers, model.cfg.n_heads, dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    for layer in tqdm(range(layers)):\n",
        "        for head_index in range(model.cfg.n_heads):\n",
        "            hook_fn = partial(patch_head_vector, head_index=head_index, cache=corrupted_cache, patch_num=0)\n",
        "            patched_logits = model.run_with_hooks(\n",
        "                clean_image.unsqueeze(0),\n",
        "                fwd_hooks=[(prisma_utils.get_act_name('z', layer, \"attn\"), hook_fn)],\n",
        "                # return_type=\"logits\",\n",
        "            )\n",
        "            patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens) + model.head.b_H[black_index] - model.head.b_H[white_index]\n",
        "\n",
        "            patched_heads[layer, head_index] = patched_logit_diff.detach()\n",
        "\n",
        "    return patched_heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zKWcWGrUkB"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f5e1e78f5311484c8f6320ea217d87c0",
            "dec07bf7418142cdb81e9b29f4a116c7",
            "aed3899b40c14f44b23767c3b002da55",
            "5b5695de8d67454f919c7891cc958144",
            "9835d6e564d5496badc2f29ba091834e",
            "4abda3a4c7a94a54a3bd190195dc8230",
            "534893d04db148e1a2231d66be0c9471",
            "d9f5628112f34d9196ba4c680ed67620",
            "cd7d227c615349f09ae087217345a132",
            "8f103aef1e064df2bc7722b62c54afb9",
            "5ac26d0edd074ee0942edb606cc8783a"
          ]
        },
        "id": "2yWoZV9W4e-R",
        "outputId": "16471cfd-ef43-4aa9-ef9e-e92255476516"
      },
      "outputs": [],
      "source": [
        "patched_heads = run_all_attn_heads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patched_heads.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "A5RjANQ75SIQ",
        "outputId": "eb4c7b41-b4b1-4f6f-d38d-115477f1ebe4"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    patched_heads,\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
        "    title=\"New Logit Difference from Each Patched Head\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWdO1I8e5sS6"
      },
      "source": [
        "Patching just Head 4 of Layer 11 (light blue) shifted the prediction from White to Black! Let's zoom in on this attention head..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F0NmUS651bk"
      },
      "source": [
        "## Breaking down one attention head (Head 4, Layer 11)\n",
        "\n",
        "Let's zoom in on Attention Head 4 of Layer 11, which has shifted the prediction to Border Collie when we patch it.\n",
        "\n",
        "Attention is a multi-step computation, so which part of the attention head is making this switch? And which patch of the original input? Is there a difference in result when we activation-patch the CLS token, vs a spatial patch (e.g. the cat's face)?\n",
        "\n",
        "Let's check out the z matrix of the Attention Head. This is the part of the residual stream that's been matmuled with the value weight matrix and then weighted by the attention pattern:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "z = self.hook_z(\n",
        "            einsum(\n",
        "                \"batch key_pos head_index d_head, \\\n",
        "                batch head_index query_pos key_pos -> \\\n",
        "                batch query_pos head_index d_head\",\n",
        "                v,\n",
        "                pattern,\n",
        "            )\n",
        "        )\n",
        "```\n",
        "From Prisma repo. Source [here](https://github.com/soniajoseph/ViT-Prisma/blob/dd3b1d456ddd1f293c06c3bf7707c05d61049bc2/src/vit_prisma/models/layers/attention.py#L268C9-L276C10).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a2caac2b51204102815a8322b6957259",
            "59692372424f42d2a3c301b4312d6356",
            "942a98280e2f495c9ae7aee666bba64f",
            "2612b0c15d614f5d98e339f14329cdf2",
            "dbbdc925c3834a61a4e64f51528f9c98",
            "7fcc03e150cd4f3b912af4838c25b06d",
            "f76f7a66a2254dec8b16f42172facacb",
            "70103c0343bf4344988d96718d61c8ee",
            "d70227d780ae437e88537a3ce552fa71",
            "bd20b10e94864440aa6716e46b508c90",
            "095b850b94d74a83a7850ffdd5591892"
          ]
        },
        "id": "VcigxxnG3Glb",
        "outputId": "c007199f-85f2-4b1c-9ae8-8cceae04686f"
      },
      "outputs": [],
      "source": [
        "patched_z = run_patch('z')\n",
        "fig = plot_patched_component(patched_z, title='z')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patched_z.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ywF9yCM288g"
      },
      "source": [
        "The y-axis is each patch number, and the x-axis is the attention head number. We can see that only patching the CLS token of Attention Head 4 flips the value to Border Collie (positive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "4aBCdKUb6pXV",
        "outputId": "614607a9-42e3-49c9-ea2f-de59e5cfc6c3"
      },
      "outputs": [],
      "source": [
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwrrffWa3d7D"
      },
      "source": [
        "Let's check out value matrix (which is computed upstream of the z matrix above). Interestingly, patching the 26th patch shifts the decision most away from tabby cat and toward Border Collie (but not enough to fully flip it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2d229c5c4396484ba4edea6302a06eae",
            "9a052eb9e8584b55ba14909a959013e4",
            "90fdb0d945e84f1f9ac4d9098f0e6b69",
            "9c808cbade1848849f05a64c761c4582",
            "0893139a06cb465c859b040e2d516899",
            "5b88ec8f0d3843d7928068016d735339",
            "28ea3c2471804541b06a6ab3dbeb8796",
            "3d23b969af6f4018b539d3408417dea5",
            "c0400b81c5a24797a127c4dc54c1894b",
            "12b760cc781c43f3a273f116a467d864",
            "fe74575799744b9d913ceccb517aff35"
          ]
        },
        "id": "pvbzIvxi3IP4",
        "outputId": "8fb3ea91-e6c9-44df-f8ca-3ed43ab7625e"
      },
      "outputs": [],
      "source": [
        "patched_v = run_patch('v')\n",
        "fig = plot_patched_component(patched_v, title='v')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "I44umIPZ6lGh",
        "outputId": "39484a98-f5c7-47b5-bb38-1a89577d9a20"
      },
      "outputs": [],
      "source": [
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoWTdtJe3wKj"
      },
      "source": [
        "Looking at the grid below, Patch 26 corresponds to the bottom part of the cat's face! So it intuitively checks out that activation patching this patch with the black crop-out would nudge the computation toward Border Collie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "LmStmmYM-s8d",
        "outputId": "a5f88042-3484-47f8-d531-ebb923ca145f"
      },
      "outputs": [],
      "source": [
        "display_grid_on_image(corrupted_image, patch_size=32)\n",
        "display_grid_on_image(clean_image, patch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMBzfDsU3-Rf"
      },
      "source": [
        "Let's now check out patching the key matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0856a403fb924eabb95dcc11eaf3413b",
            "4c56c48f02de44fbaffd4354c30d6a9e",
            "c83eba6d1b544bb19aa47236f8fa1c32",
            "e8d86da2917d4d2582bebbe1a25ff626",
            "1d7cd9d102014d9c870f62832dd4dac1",
            "1cb58771475546f39eda2400c7ebb7af",
            "5f46e50a87e143bbb859deaf9da6e460",
            "6a01c7790b54445fa055601ff10c1a2a",
            "ea34793968d4491c9696db892c448916",
            "c82f232138854a4bb22a4b051410683b",
            "0519020444eb4cbc8e2dc455b1199cea"
          ]
        },
        "id": "4n3qPKBS3ZKf",
        "outputId": "a1b94045-9d1c-4adb-e53c-d9fe5ee1cdf7"
      },
      "outputs": [],
      "source": [
        "patched_k = run_patch('k')\n",
        "fig = plot_patched_component(patched_k, title='k')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJgKeVGS4AjV"
      },
      "source": [
        "Patch 26 also makes a difference! But not enough to flip the computation by itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "DwxEpE8P6sac",
        "outputId": "27397586-a5b8-48c3-c4ca-dd38fb11708c"
      },
      "outputs": [],
      "source": [
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiut2zOT4ICA"
      },
      "source": [
        "Let's run the query matrix to finish off this attention head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "529f41b676d54356bf925ff10e1193e9",
            "0a108515aab84c93b0e19c5159710273",
            "d62e2a364af54d2ca37067b555798981",
            "75555ee8c4054426bb74f33fbd9fdf09",
            "a600a82d06de4f04ad247f9621bfb062",
            "004646fe041547f6893865ecc651cc09",
            "8a7c04267d04495598d0756f59eafe1e",
            "0c45d03695174c008ea6afb03c2bf6b1",
            "f7efad1e6e7f4f619521c545ee497298",
            "670299e08a964ff0be4c3939c53fb7f9",
            "52ffb2b2b63f49ad9b88fbd356e5725f"
          ]
        },
        "id": "-znhyBtf3b0-",
        "outputId": "fa80012a-dde2-4a89-9eb9-352b4934a7c0"
      },
      "outputs": [],
      "source": [
        "patched_q = run_patch('q')\n",
        "fig = plot_patched_component(patched_q, title='q')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "PfQO2-Zd6tfP",
        "outputId": "ad19f6b0-f38d-43ca-a8ea-ea273fb60456"
      },
      "outputs": [],
      "source": [
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS8hoXwH4M0A"
      },
      "source": [
        "It appears that no activation-patching in the query matrix shifts the computation too dramatically here.\n",
        "\n",
        "Putting this all together, this means that the significant cat-dog deciding computation happens at the z-matrix (when the value matrix is weighted by the attention pattern).\n",
        "\n",
        "Here, the \"cat information\" (Patch 26, the lower cat face) is being aggregated and sent to the CLS token.\n",
        "\n",
        "Can you visualize key-query heatmaps to better visualize why?\n",
        "\n",
        "Finally, let's plot v vs z of every patch/head combo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBjjMs7q6TvR"
      },
      "source": [
        "### Plotting code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBVNVzuZ6TZp"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_heads = model.cfg.n_heads\n",
        "n_patches = 50  # Assuming there are 50 patches\n",
        "\n",
        "# Generate combined labels for patches and heads\n",
        "combined_labels = [f\"Patch {p} H{h}\" for p in range(n_patches) for h in range(n_heads)]\n",
        "\n",
        "colors = np.tile(np.arange(n_heads), n_patches)\n",
        "\n",
        "x_values, y_values = patched_v.flatten(), patched_z.flatten()\n",
        "\n",
        "# Create the scatter plot\n",
        "fig = px.scatter(\n",
        "    x=x_values,\n",
        "    y=y_values,\n",
        "    # colors=colors,\n",
        "    labels={\n",
        "        \"x\": \"v Patch Difference\",\n",
        "        \"y\": \"z Patch Difference\",\n",
        "        \"color\": \"Layer & Patch\"\n",
        "    },\n",
        "    title=\"Scatter plot of z patching vs v patching\",\n",
        "\n",
        ")\n",
        "\n",
        "# Add hover data for each point with custom labels\n",
        "fig.update_traces(marker=dict(size=6),\n",
        "                  selector=dict(mode='markers'),\n",
        "                  hovertemplate=[\"%{x}<br>%{y}<br>%{text}\" for _ in combined_labels],\n",
        "                  text=combined_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLpkUvc86W65"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "PoINRJc06t6s",
        "outputId": "edd92a49-d74e-4991-9a7c-650b9512dd74"
      },
      "outputs": [],
      "source": [
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Ylrdq24yfG"
      },
      "source": [
        "It appears that the value activation patching makes a more significant difference for some patches (e.g. Patch 26) than the CLS otken, where the output patch matters more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du1nWG_v7JcN"
      },
      "source": [
        "## Run net again with minimum viable ablation\n",
        "\n",
        "Let's flip the net's prediction from tabby cat to Border collie by changing as little as we can.\n",
        "\n",
        "Note that we are only patching the CLS token of the z matrix of Attention Head 4, Layer 11. This is a tiny ablation (a 64-component vector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y298F0sB5km2"
      },
      "outputs": [],
      "source": [
        "def patch_head_vector(\n",
        "    original_head_vector,\n",
        "    hook,\n",
        "    head_index,\n",
        "    cache,\n",
        "    patch_num\n",
        "):\n",
        "\n",
        "    original_head_vector[:, patch_num, head_index, :] = cache[hook.name][\n",
        "        :, patch_num, head_index, :\n",
        "    ]\n",
        "    return original_head_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTltiNcP7Kf1",
        "outputId": "bf549491-693d-4c76-c740-9371458e209e"
      },
      "outputs": [],
      "source": [
        "# from vit_prisma.utils.data_utils.imagenet_utils import race_index_from_word\n",
        "from vit_prisma.utils.data_utils.race_dict import RACE_DICT\n",
        "\n",
        "\n",
        "hook_fn = partial(patch_head_vector, head_index=4, cache=corrupted_cache, patch_num=0)\n",
        "logits = model.run_with_hooks(\n",
        "    clean_image.unsqueeze(0),\n",
        "    fwd_hooks=[(prisma_utils.get_act_name('z', 11, \"attn\"), hook_fn)],\n",
        ")\n",
        "\n",
        "probs = logits.softmax(dim=-1)\n",
        "probs = probs.squeeze(0).detach().numpy()\n",
        "sorted_probs = np.sort(probs)[::-1]\n",
        "sorted_probs_args = np.argsort(probs)[::-1]\n",
        "\n",
        "top_k = 3\n",
        "for i in range(top_k):\n",
        "    index = sorted_probs_args[i]\n",
        "    prob = sorted_probs[i]\n",
        "    logit = logits[0, index].item()  # Assuming you want to show the original logit value\n",
        "    label = RACE_DICT[index]\n",
        "\n",
        "    rank_str = f\"Top {i}th token.\"\n",
        "    logit_str = f\"Logit: {logit:.2f}\"\n",
        "    prob_str = f\"Prob: {prob * 100:.2f}%\"\n",
        "    token_str = f\"Label: |{label}|\"\n",
        "    print(f\"{rank_str} {logit_str} {prob_str} {token_str}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS1_Yr6S83qc"
      },
      "source": [
        "The net has shifted its classification from tabby cat to Border collie. Tabby cat is now the 5th answer. And we got the network to switch by making the Minimum Viable Ablation: this is just the CLS token of the z matrix of Head 4, Layer 11! We altered the net with surgical precision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DpZRHN87N7r"
      },
      "source": [
        "# Outro\n",
        "\n",
        "## More problems\n",
        "\n",
        "* Can you make the ablation *even more* precise by ablating the features of the CLS embedding, instead of the whole embedding?\n",
        "* Does this result generalize to other cat/dog images? Try flipping the image.\n",
        "* Can you figure out exactly how the CLS is accruing information from the other patches, including Patch 26?\n",
        "\n",
        "## More notebooks\n",
        "\n",
        "* [Emoji Logit Lens Demo](https://colab.research.google.com/drive/1yAHrEoIgkaVqdWC4GY-GQ46ZCnorkIVo)\n",
        "* [Interactive Attention Head Demo](https://colab.research.google.com/drive/1P252fCvTHNL_yhqJDeDVOXKCzIgIuAz2)\n",
        "\n",
        "See the [Prisma repo](https://github.com/soniajoseph/ViT-Prisma/tree/main) for the original codebase.\n",
        "\n",
        "*If you have feedback, please DM me on the LW forum or Twitter (@soniajoseph_).*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMiCSDNO6cL1q5pVfuuVfVI",
      "collapsed_sections": [
        "nRmhq7T8iObj",
        "SBjjMs7q6TvR"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004646fe041547f6893865ecc651cc09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009df655df6e4edbb5de9a7a318ef411": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0519020444eb4cbc8e2dc455b1199cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0856a403fb924eabb95dcc11eaf3413b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c56c48f02de44fbaffd4354c30d6a9e",
              "IPY_MODEL_c83eba6d1b544bb19aa47236f8fa1c32",
              "IPY_MODEL_e8d86da2917d4d2582bebbe1a25ff626"
            ],
            "layout": "IPY_MODEL_1d7cd9d102014d9c870f62832dd4dac1"
          }
        },
        "0893139a06cb465c859b040e2d516899": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095b850b94d74a83a7850ffdd5591892": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a108515aab84c93b0e19c5159710273": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004646fe041547f6893865ecc651cc09",
            "placeholder": "",
            "style": "IPY_MODEL_8a7c04267d04495598d0756f59eafe1e",
            "value": "100%"
          }
        },
        "0c45d03695174c008ea6afb03c2bf6b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12b760cc781c43f3a273f116a467d864": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cb58771475546f39eda2400c7ebb7af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d79d72f8b6b4a0cb2407a3f555ee1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e7d2706e772426ca3bf653b6fafe91f",
              "IPY_MODEL_ebeaf9ba22934b20ac81fe2770c1ec03",
              "IPY_MODEL_fec789e2132d4878bdb4935ee35c82c6"
            ],
            "layout": "IPY_MODEL_a27d66e732e2492e832f2326f3252908"
          }
        },
        "1d7cd9d102014d9c870f62832dd4dac1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "210a1f02a58f4e948c5ad424e258a9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2442c32974784ece88322b0c107ddf10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2612b0c15d614f5d98e339f14329cdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd20b10e94864440aa6716e46b508c90",
            "placeholder": "",
            "style": "IPY_MODEL_095b850b94d74a83a7850ffdd5591892",
            "value": "50/50[01:01&lt;00:00,1.14s/it]"
          }
        },
        "28ea3c2471804541b06a6ab3dbeb8796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d229c5c4396484ba4edea6302a06eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a052eb9e8584b55ba14909a959013e4",
              "IPY_MODEL_90fdb0d945e84f1f9ac4d9098f0e6b69",
              "IPY_MODEL_9c808cbade1848849f05a64c761c4582"
            ],
            "layout": "IPY_MODEL_0893139a06cb465c859b040e2d516899"
          }
        },
        "3d23b969af6f4018b539d3408417dea5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4161dece78b147d893b036997d5bffcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6ef2818f95249a59e75c9bbadee0f5c",
            "max": 352911016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ea7ed3625b74c5d83308309e8358feb",
            "value": 352911016
          }
        },
        "4abda3a4c7a94a54a3bd190195dc8230": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c56c48f02de44fbaffd4354c30d6a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cb58771475546f39eda2400c7ebb7af",
            "placeholder": "",
            "style": "IPY_MODEL_5f46e50a87e143bbb859deaf9da6e460",
            "value": "100%"
          }
        },
        "5049b576415a4ef4b5c9382b0819d440": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "529f41b676d54356bf925ff10e1193e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a108515aab84c93b0e19c5159710273",
              "IPY_MODEL_d62e2a364af54d2ca37067b555798981",
              "IPY_MODEL_75555ee8c4054426bb74f33fbd9fdf09"
            ],
            "layout": "IPY_MODEL_a600a82d06de4f04ad247f9621bfb062"
          }
        },
        "52f59766c812422288ae91e2cbfdf95f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ffb2b2b63f49ad9b88fbd356e5725f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "534893d04db148e1a2231d66be0c9471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59692372424f42d2a3c301b4312d6356": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fcc03e150cd4f3b912af4838c25b06d",
            "placeholder": "",
            "style": "IPY_MODEL_f76f7a66a2254dec8b16f42172facacb",
            "value": "100%"
          }
        },
        "5ac26d0edd074ee0942edb606cc8783a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ad89348d26c4c7185f5358f45b7a04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b5695de8d67454f919c7891cc958144": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f103aef1e064df2bc7722b62c54afb9",
            "placeholder": "",
            "style": "IPY_MODEL_5ac26d0edd074ee0942edb606cc8783a",
            "value": "12/12[00:15&lt;00:00,1.18s/it]"
          }
        },
        "5b88ec8f0d3843d7928068016d735339": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f46e50a87e143bbb859deaf9da6e460": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62752397a68b465e84032f72f3cca0cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6428af5766574e5fa07c1348a1965be9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670299e08a964ff0be4c3939c53fb7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a01c7790b54445fa055601ff10c1a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70103c0343bf4344988d96718d61c8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75555ee8c4054426bb74f33fbd9fdf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_670299e08a964ff0be4c3939c53fb7f9",
            "placeholder": "",
            "style": "IPY_MODEL_52ffb2b2b63f49ad9b88fbd356e5725f",
            "value": "50/50[01:01&lt;00:00,1.15s/it]"
          }
        },
        "7fcc03e150cd4f3b912af4838c25b06d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a7c04267d04495598d0756f59eafe1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e7d2706e772426ca3bf653b6fafe91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9fb88435e7e4c5f89cc915b7a0856fd",
            "placeholder": "",
            "style": "IPY_MODEL_bbc3cbe3b9d44b6d9b93a047a11334ad",
            "value": "config.json:100%"
          }
        },
        "8f103aef1e064df2bc7722b62c54afb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90fdb0d945e84f1f9ac4d9098f0e6b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d23b969af6f4018b539d3408417dea5",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0400b81c5a24797a127c4dc54c1894b",
            "value": 50
          }
        },
        "942a98280e2f495c9ae7aee666bba64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70103c0343bf4344988d96718d61c8ee",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d70227d780ae437e88537a3ce552fa71",
            "value": 50
          }
        },
        "9835d6e564d5496badc2f29ba091834e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a052eb9e8584b55ba14909a959013e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b88ec8f0d3843d7928068016d735339",
            "placeholder": "",
            "style": "IPY_MODEL_28ea3c2471804541b06a6ab3dbeb8796",
            "value": "100%"
          }
        },
        "9c808cbade1848849f05a64c761c4582": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12b760cc781c43f3a273f116a467d864",
            "placeholder": "",
            "style": "IPY_MODEL_fe74575799744b9d913ceccb517aff35",
            "value": "50/50[00:58&lt;00:00,1.15s/it]"
          }
        },
        "9ea7ed3625b74c5d83308309e8358feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a15fd9308e7c4d799cf4622d40ca70d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f59766c812422288ae91e2cbfdf95f",
            "placeholder": "",
            "style": "IPY_MODEL_2442c32974784ece88322b0c107ddf10",
            "value": "model.safetensors:100%"
          }
        },
        "a27d66e732e2492e832f2326f3252908": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2caac2b51204102815a8322b6957259": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59692372424f42d2a3c301b4312d6356",
              "IPY_MODEL_942a98280e2f495c9ae7aee666bba64f",
              "IPY_MODEL_2612b0c15d614f5d98e339f14329cdf2"
            ],
            "layout": "IPY_MODEL_dbbdc925c3834a61a4e64f51528f9c98"
          }
        },
        "a600a82d06de4f04ad247f9621bfb062": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a875e084c4c44bce8c2ece41e946b859": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a15fd9308e7c4d799cf4622d40ca70d5",
              "IPY_MODEL_4161dece78b147d893b036997d5bffcb",
              "IPY_MODEL_c59ba6dbe4554429806f21ea948cc56d"
            ],
            "layout": "IPY_MODEL_009df655df6e4edbb5de9a7a318ef411"
          }
        },
        "aed3899b40c14f44b23767c3b002da55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9f5628112f34d9196ba4c680ed67620",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd7d227c615349f09ae087217345a132",
            "value": 12
          }
        },
        "b9fb88435e7e4c5f89cc915b7a0856fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbc3cbe3b9d44b6d9b93a047a11334ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd20b10e94864440aa6716e46b508c90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0400b81c5a24797a127c4dc54c1894b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c59ba6dbe4554429806f21ea948cc56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210a1f02a58f4e948c5ad424e258a9e0",
            "placeholder": "",
            "style": "IPY_MODEL_5ad89348d26c4c7185f5358f45b7a04f",
            "value": "353M/353M[00:18&lt;00:00,20.8MB/s]"
          }
        },
        "c6ef2818f95249a59e75c9bbadee0f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82f232138854a4bb22a4b051410683b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83eba6d1b544bb19aa47236f8fa1c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a01c7790b54445fa055601ff10c1a2a",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea34793968d4491c9696db892c448916",
            "value": 50
          }
        },
        "cd7d227c615349f09ae087217345a132": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d41dca76979a4e2e8fda8e05b50a222b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d62e2a364af54d2ca37067b555798981": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c45d03695174c008ea6afb03c2bf6b1",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7efad1e6e7f4f619521c545ee497298",
            "value": 50
          }
        },
        "d70227d780ae437e88537a3ce552fa71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9f5628112f34d9196ba4c680ed67620": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbbdc925c3834a61a4e64f51528f9c98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec07bf7418142cdb81e9b29f4a116c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4abda3a4c7a94a54a3bd190195dc8230",
            "placeholder": "",
            "style": "IPY_MODEL_534893d04db148e1a2231d66be0c9471",
            "value": "100%"
          }
        },
        "e8d86da2917d4d2582bebbe1a25ff626": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c82f232138854a4bb22a4b051410683b",
            "placeholder": "",
            "style": "IPY_MODEL_0519020444eb4cbc8e2dc455b1199cea",
            "value": "50/50[00:57&lt;00:00,1.16s/it]"
          }
        },
        "ea34793968d4491c9696db892c448916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebeaf9ba22934b20ac81fe2770c1ec03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62752397a68b465e84032f72f3cca0cc",
            "max": 586,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d41dca76979a4e2e8fda8e05b50a222b",
            "value": 586
          }
        },
        "f5e1e78f5311484c8f6320ea217d87c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dec07bf7418142cdb81e9b29f4a116c7",
              "IPY_MODEL_aed3899b40c14f44b23767c3b002da55",
              "IPY_MODEL_5b5695de8d67454f919c7891cc958144"
            ],
            "layout": "IPY_MODEL_9835d6e564d5496badc2f29ba091834e"
          }
        },
        "f76f7a66a2254dec8b16f42172facacb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7efad1e6e7f4f619521c545ee497298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe74575799744b9d913ceccb517aff35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fec789e2132d4878bdb4935ee35c82c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6428af5766574e5fa07c1348a1965be9",
            "placeholder": "",
            "style": "IPY_MODEL_5049b576415a4ef4b5c9382b0819d440",
            "value": "586/586[00:00&lt;00:00,51.7kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
